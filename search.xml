<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>hadoop完全分布式安装</title>
      <link href="/2021/12/26/hadoop-wan-quan-fen-bu-shi-an-zhuang/"/>
      <url>/2021/12/26/hadoop-wan-quan-fen-bu-shi-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h1 id="hadoop完全分布式安装"><a href="#hadoop完全分布式安装" class="headerlink" title="hadoop完全分布式安装"></a>hadoop完全分布式安装</h1><h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><h3 id="1-1-需要准备的文件"><a href="#1-1-需要准备的文件" class="headerlink" title="1.1  需要准备的文件"></a>1.1  需要准备的文件</h3><p>hadoop-2.7.7.tar.gz、jdk-8u191-linux-x64.tar.gz压缩包</p><h3 id="1-2-创建Dockerfile文件"><a href="#1-2-创建Dockerfile文件" class="headerlink" title="1.2 创建Dockerfile文件"></a>1.2 创建Dockerfile文件</h3><p>[root@master azl ] # vim Dockerfile</p><figure class="highlight dockerfile"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选择centos7.7.1908作为基础镜像</span></span><br><span class="line"><span class="keyword">FROM</span> centos:centos7.<span class="number">7.1908</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#镜像维护者信息</span></span><br><span class="line"><span class="keyword">MAINTAINER</span> anzhaoliang1@<span class="number">163</span>.com</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建容器时需要运行的命令</span></span><br><span class="line"><span class="comment">#安装openssh-server. openssh-clents. sudo、 vim和net-tools软件包</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum install -y  openssh-server openssh-clients sudo vim net-tools</span></span><br><span class="line"><span class="comment">#生成相应的主机密钥文件</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建自定义组和用户、设置密码并授予root权限</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> groupadd -g 1124 bigdata &amp;&amp; useradd -m -u 1124 -g bigdata xiaoan</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"xiaoan:root"</span> | chpasswd</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"xiaoan ALL=(ALL)       NOPASSWD:ALL"</span> &gt;&gt; /etc/sudoers</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建模块和软件目录并修改权限</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir /opt/software &amp;&amp; mkdir /opt/moudle</span></span><br><span class="line"><span class="comment">#将宿主机的文件拷贝至境像(ADD会自动解压)</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> jdk-8u301-linux-aarch64.tar.gz /opt/moudle</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> hadoop-2.7.7.tar.gz /opt/software</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> scala-2.11.12.tgz /opt/moudle</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> spark-2.3.3-bin-hadoop2.7.tgz /opt/software</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> chown -R xiaoan:bigdata /opt/moudle &amp;&amp; chown -R xiaoan:bigdata /opt/software</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置环境变量</span></span><br><span class="line"><span class="keyword">ENV</span> CENTOS_DEFAULT_HOME /root</span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME /opt/moudle/jdk1.<span class="number">8.0</span>_301</span><br><span class="line"><span class="keyword">ENV</span> HADOOP_HOME /opt/software/hadoop-<span class="number">2.7</span>.<span class="number">7</span></span><br><span class="line"><span class="keyword">ENV</span> SCALA_HOME /opt/moudle/scala-<span class="number">2.11</span>.<span class="number">12</span></span><br><span class="line"><span class="keyword">ENV</span> SPARK_HOME /opt/software/spark-<span class="number">2.3</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span></span><br><span class="line"><span class="keyword">ENV</span> JRE_HOME ${JAVA_HOME}/jre</span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH ${JAVA_HOME}/lib:${JRE_HOME}/lib</span><br><span class="line"><span class="keyword">ENV</span> PATH ${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SCALA_HOME}/bin:${SPARK_HOME}/bin:$PATH</span><br><span class="line"><span class="comment">#终端默认登录进来的工作目录</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$CENTOS_DEFAULT_HOME</span></span></span><br><span class="line"><span class="comment">#启动sshd服务并且暴露22端口</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">22</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/usr/sbin/sshd"</span>, <span class="string">"-D"</span>]</span></span><br></pre></td></tr></tbody></table></figure><h3 id="1-3-编译docker文件"><a href="#1-3-编译docker文件" class="headerlink" title="1.3  编译docker文件"></a>1.3  编译docker文件</h3><p>[root@master azl ] # docker build -t azl/hadoop:2.7.7 . </p><h3 id="1-4-设置网络"><a href="#1-4-设置网络" class="headerlink" title="1.4  设置网络"></a>1.4  设置网络</h3><p>[root@master azl ] # docker network ls</p><p><img src="/source/images/01.png" alt="查看网络"></p><p>[root@master azl ] # docker network inspect [容器id]</p><p><img src="/source/images/02.png" alt="查看网络id"></p><h4 id="1-4-1-创建自己的网络"><a href="#1-4-1-创建自己的网络" class="headerlink" title="1.4.1 创建自己的网络"></a>1.4.1 创建自己的网络</h4><p>[root@master azl ] # docker network create –subnet=172.22.0.0/24 mynetwork（mynework网络名随便取）</p><p><img src="/source/images/03.png" alt="image-20210808211854087"></p><h4 id="1-4-2-查看镜像"><a href="#1-4-2-查看镜像" class="headerlink" title="1.4.2 查看镜像"></a>1.4.2 查看镜像</h4><p>[root@master azl ] # docker images </p><p><img src="/source/images/04.png" alt="image-20210808211953388"></p><h2 id="二、集群规划"><a href="#二、集群规划" class="headerlink" title="二、集群规划"></a>二、集群规划</h2><p>这里搭建一个 5个节点的 Hadoop 集群，其中四台主机均部署 <code>DataNode</code> 和 <code>NodeManager</code> 服务，但其中 hadoop02 上部署 <code>SecondaryNameNode</code>服务和NTP服务器，hadoop01 上部署 <code>NameNode</code> 、 <code>ResourceManager</code> 和<code>JobHistoryServer</code> 服务。</p><p><img src="/source/images/05.jpg"></p><p>节点ip分配情况如下：</p><p>hadoop01=172.16.0.2</p><p>hadoop02=172.16.0.3</p><p>hadoop03=172.16.0.4</p><p>hadoop04=172.16.0.5</p><p>hadoop05=172.16.0.6</p><h2 id="三、创建并启动容器"><a href="#三、创建并启动容器" class="headerlink" title="三、创建并启动容器"></a>三、创建并启动容器</h2><h3 id="3-1-启动5个容器"><a href="#3-1-启动5个容器" class="headerlink" title="3.1 启动5个容器"></a>3.1 启动5个容器</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]#docker run -d --name hadoop01 --hostname hadoop01 -P -p 50070:50070 -p 8088:8088 -p 19888:19888 --privileged 06807f94352b /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop02 --hostname hadoop02 -P -p 50090:50090 --privileged 4875c676be38 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop03 --hostname hadoop03 -P --privileged 4e26523f8b9f /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop04 --hostname hadoop04 -P --privileged 307c55207976 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop05 --hostname hadoop05 -P --privileged 6951ef7c0997 /usr/sbin/init</span><br></pre></td></tr></tbody></table></figure><p>复制五个终端，以之前创建的镜像启动5个容器</p><ul><li><h6 id="手动启动"><a href="#手动启动" class="headerlink" title="手动启动"></a>手动启动</h6></li></ul><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]#docker run -d --name hadoop01 --hostname hadoop01 -P -p 50070:50070 -p 8088:8088 -p 19888:19888 -p 8080:8080 -p 4040:4040 -p 18080:18080 -p 9000:9000 --privileged 8bf3810dea30 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop02 --hostname hadoop02 -P -p 50090:50090 --privileged 3d9b697593df /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop03 --hostname hadoop03 -P --privileged 48dedc962fd7 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop04 --hostname hadoop04 -P --privileged 7812c22374b3 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">[root@master ~]#docker run -d --name hadoop05 --hostname hadoop05 --privileged 87ce5105b70d /usr/sbin/init</span><br></pre></td></tr></tbody></table></figure><ul><li>脚本启动</li></ul><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">description：Batch start Containers Script</span></span><br><span class="line"><span class="meta">#</span><span class="bash">author：anzhaoliang</span></span><br><span class="line">if [ $# -ne 1 ];then</span><br><span class="line">    echo "You need to start serveral containers explicitly."</span><br><span class="line">    echo "Some like "./hadoop-cluster.sh 3" or "sh hadoop-cluster.sh 3""</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">要启动的容器数量</span></span><br><span class="line">NUM_CONTAINERS=$1</span><br><span class="line"><span class="meta">#</span><span class="bash">自定义网络名称</span></span><br><span class="line">NETWORK_NAME=mynetwork</span><br><span class="line"><span class="meta">#</span><span class="bash">镜像ID</span></span><br><span class="line">IMAGES_ID=4b7319b12402</span><br><span class="line"><span class="meta">#</span><span class="bash">前缀</span></span><br><span class="line">PREFIX="0"</span><br><span class="line">for (( i=1;i&lt;=$NUM_CONTAINERS;i++ ))</span><br><span class="line">do</span><br><span class="line">  if [ $i -eq 1 ];then</span><br><span class="line">                     sudo docker run -d --name hadoop$PREFIX$i --hostname hadoop$PREFIX$i --net ${NETWORK_NAME} --ip 172.22.0.$[$i+1] -P -p 50070:50070 -p 8088:8088 -p 19888:19888 -p 8080:8080 -p 4040:4040 --privileged $IMAGES_ID /usr/sbin/init</span><br><span class="line">               elif [ $i -eq 2 ];then</span><br><span class="line">                      sudo docker run -d --name hadoop$PREFIX$i --hostname hadoop$PREFIX$i --net ${NETWORK_NAME} --ip 172.22.0.$[$i+1] -P -p 50090:50090 --privileged $IMAGES_ID /usr/sbin/init</span><br><span class="line">               else</span><br><span class="line">                      sudo docker run -d --name hadoop04 --hostname hadoop04 --net ${NETWORK_NAME} --ip 172.22.0.$[$i+1] -P --privileged $IMAGES_ID /usr/sbin/init</span><br><span class="line">            fi</span><br><span class="line">        done</span><br><span class="line">echo "$NUM_CONTAINERS containers started!"</span><br><span class="line">echo "==================================="</span><br><span class="line">sudo docker ps | grep hadoop</span><br><span class="line">sudo docker inspect $(sudo docker ps -q) | grep -i ipv4</span><br><span class="line">echo "==================================="</span><br><span class="line">echo "------Start Success------"</span><br></pre></td></tr></tbody></table></figure><h2 id="四、容器内进行集群配置"><a href="#四、容器内进行集群配置" class="headerlink" title="四、容器内进行集群配置"></a>四、容器内进行集群配置</h2><h3 id="4-1-进入容器"><a href="#4-1-进入容器" class="headerlink" title="4.1  进入容器"></a>4.1  进入容器</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master azl ] # docker exec -it e8c0119dab34 /bin/bash</span><br><span class="line">[root@master azl ] # docker exec -it 083a2c661cb2 /bin/bash</span><br><span class="line">[root@master azl ] # docker exec -it 52a23434fe58 /bin/bash</span><br><span class="line">[root@master azl ] # docker exec -it 8e0f17464ddf /bin/bash</span><br><span class="line">[root@master azl ] # docker exec -it 8e0f17464ddf /bin/bash</span><br></pre></td></tr></tbody></table></figure><h3 id="4-2-切换到用户目录-所有容器全部执行"><a href="#4-2-切换到用户目录-所有容器全部执行" class="headerlink" title="4.2 切换到用户目录(所有容器全部执行)"></a>4.2 切换到用户目录(所有容器全部执行)</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01~ ]$ su anzhaoliang</span><br><span class="line">[anzhaoliang@hadoop01 root]$ cd</span><br><span class="line">[anzhaoliang@hadoop01 ~]$</span><br></pre></td></tr></tbody></table></figure><h3 id="4-3-配置映射-所有容器全部执行"><a href="#4-3-配置映射-所有容器全部执行" class="headerlink" title="4.3 配置映射(所有容器全部执行)"></a>4.3 配置映射(所有容器全部执行)</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ sudo vim /etc/hosts</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">172.22.0.2 hadoop01</span><br><span class="line">172.22.0.3 hadoop02</span><br><span class="line">172.22.0.4 hadoop03</span><br><span class="line">172.22.0.5 hadoop04</span><br><span class="line">172.22.0.6 hadoop05</span><br></pre></td></tr></tbody></table></figure><h2 id="五、配置主从节点免密登录"><a href="#五、配置主从节点免密登录" class="headerlink" title="五、配置主从节点免密登录"></a>五、配置主从节点免密登录</h2><h3 id="5-1-生成密钥"><a href="#5-1-生成密钥" class="headerlink" title="5.1  生成密钥"></a>5.1  生成密钥</h3><p>在每台主机上使用ssh-keygen 命令生成公钥私钥对（五台机器全部执行）</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ ssh-keygen -t rsa</span><br></pre></td></tr></tbody></table></figure><p>一路回车</p><h3 id="5-2-复制公钥"><a href="#5-2-复制公钥" class="headerlink" title="5.2  复制公钥"></a>5.2  复制公钥</h3><p>将hadoop01的公钥写到本机和远程机器的~/.ssh/authorized_keys文件中（五台机器全部执行）</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop01</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop02</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop03</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop04</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop05</span><br></pre></td></tr></tbody></table></figure><h3 id="5-3-验证免密登录"><a href="#5-3-验证免密登录" class="headerlink" title="5.3 验证免密登录"></a>5.3 验证免密登录</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ ssh hadoop01</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh hadoop02</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh hadoop03</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh hadoop04</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ ssh hadoop05</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-配置时间服务器（hadoop02上）"><a href="#5-4-配置时间服务器（hadoop02上）" class="headerlink" title="5.4 配置时间服务器（hadoop02上）"></a>5.4 配置时间服务器（hadoop02上）</h3><h4 id="5-4-1-检查ntp包是否安装（每个节点都执行）"><a href="#5-4-1-检查ntp包是否安装（每个节点都执行）" class="headerlink" title="5.4.1 检查ntp包是否安装（每个节点都执行）"></a>5.4.1 检查ntp包是否安装（每个节点都执行）</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop02 ~]$ rpm -qa | grep ntp</span><br><span class="line">[anzhaoliang@hadoop02 ~]$</span><br><span class="line"><span class="meta">#</span><span class="bash">没有安装的话，执行以下命令进行安装</span></span><br><span class="line">[anzhaoliang@hadoop02 ~]$ sudo yum -y install ntp</span><br></pre></td></tr></tbody></table></figure><h4 id="5-4-2-设置时间配置文件-hadoop02"><a href="#5-4-2-设置时间配置文件-hadoop02" class="headerlink" title="5.4.2 设置时间配置文件(hadoop02)"></a>5.4.2 设置时间配置文件(hadoop02)</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop02 ~]$ sudo vim /etc/ntp.conf</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">修改一（设置本地网络上的主机不受限制）</span></span><br><span class="line"><span class="meta">#</span><span class="bash">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 为</span></span><br><span class="line">restrict 192.168.239.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"><span class="meta">#</span><span class="bash">修改二（添加默认的一个内部时钟数据，使用它为局域网用户提供服务）</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line"><span class="meta">#</span><span class="bash">修改三（设置为不采用公共的服务器）</span></span><br><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst 为</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br></pre></td></tr></tbody></table></figure><h4 id="5-4-3-设置BIOS与系统时间同步"><a href="#5-4-3-设置BIOS与系统时间同步" class="headerlink" title="5.4.3 设置BIOS与系统时间同步"></a>5.4.3 设置BIOS与系统时间同步</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop02 ~]$ sudo vim /etc/sysconfig/ntpd</span><br><span class="line"><span class="meta">#</span><span class="bash">增加如下内容（让硬件时间与系统时间一起同步）</span></span><br><span class="line">OPTIONS="-u ntp:ntp -p /var/run/ntpd.pid -g"</span><br><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></tbody></table></figure><h4 id="5-4-4-启动ntp服务并测试"><a href="#5-4-4-启动ntp服务并测试" class="headerlink" title="5.4.4 启动ntp服务并测试"></a>5.4.4 启动ntp服务并测试</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop02 ~]$ sudo systemctl start ntpd</span><br><span class="line">[anzhaoliang@hadoop02 ~]$ systemctl status ntpd</span><br><span class="line"><span class="meta">#</span><span class="bash">设置ntp服务开机自启</span></span><br><span class="line">[anzhaoliang@hadoop02 ~]$ sudo systemctl enable ntpd.service</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">测试</span></span><br><span class="line">[anzhaoliang@hadoop02 ~]$ ntpstat</span><br><span class="line">synchronised to local net (127.127.1.0) at stratum 11</span><br><span class="line">   time correct to within 3948 ms</span><br><span class="line">   polling server every 64 s</span><br><span class="line">    [anzhaoliang@hadoop02 ~]$ sudo ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line">*LOCAL(0)        .LOCL.          10 l   26   64    3    0.000    0.000   0.000</span><br></pre></td></tr></tbody></table></figure><h4 id="5-4-5-设置时区（全部执行）"><a href="#5-4-5-设置时区（全部执行）" class="headerlink" title="5.4.5 设置时区（全部执行）"></a>5.4.5 设置时区（全部执行）</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop02 ~]$ timedatectl</span><br><span class="line">[anzhaoliang@hadoop02 ~]$ sudo timedatectl set-timezone Asia/Shanghai</span><br><span class="line"><span class="meta">#</span><span class="bash">安装插件</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ sudo yum -y install vixie-cron crontabs</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">除hadoop02外全部修改/etc/crontab</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ sudo vim /etc/crontab</span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop02</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">立即生效</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ sudo crontab /etc/crontab</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看时间状态</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ date</span><br></pre></td></tr></tbody></table></figure><h2 id="六、修改主节点配置文件（hadoop01）"><a href="#六、修改主节点配置文件（hadoop01）" class="headerlink" title="六、修改主节点配置文件（hadoop01）"></a>六、修改主节点配置文件（hadoop01）</h2><h3 id="6-1-修改hadoop-env-sh文件"><a href="#6-1-修改hadoop-env-sh文件" class="headerlink" title="6.1  修改hadoop-env.sh文件"></a>6.1  修改hadoop-env.sh文件</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ cd /opt/software/hadoop-2.7.7/etc/hadoop/</span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ vim hadoop-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">25行 <span class="built_in">export</span> JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/moudle/jdk1.8.0_301</span><br><span class="line"><span class="meta">#</span><span class="bash">33行 <span class="built_in">export</span> HADOOP_CONF_DIR</span></span><br><span class="line">export HADOOP_CONF_DIR=/opt/software/hadoop-2.7.7/etc/hadoop</span><br></pre></td></tr></tbody></table></figure><h3 id="6-2-创建文件夹"><a href="#6-2-创建文件夹" class="headerlink" title="6.2 创建文件夹"></a>6.2 创建文件夹</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 hadoop]$ mkdir /opt/software/hadoop-2.7.7/tmp</span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ mkdir -p /opt/software/hadoop-2.7.7/dfs/namenode_data</span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ mkdir -p /opt/software/hadoop-2.7.7/dfs/datanode_data</span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ mkdir -p /opt/software/hadoop-2.7.7/checkpoint/dfs/cname</span><br></pre></td></tr></tbody></table></figure><h3 id="6-3-修改配置文件"><a href="#6-3-修改配置文件" class="headerlink" title="6.3 修改配置文件"></a>6.3 修改配置文件</h3><p>1). core-site.xml</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 hadoop]$ vim core-site.xml</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span><br><span class="line">            &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;  </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;   </span><br><span class="line">            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;file:///opt/software/hadoop-2.7.7/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--设置缓存大小，默认4kb--&gt;</span><br><span class="line">            &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></tbody></table></figure><p>2). hdfs-site.xml</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 hadoop]$ vim hdfs-site.xml</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">           &lt;!--数据块默认大小128M--&gt;</span><br><span class="line">           &lt;name&gt;dfs.block.size&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;134217728&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--副本数量，不配置的话默认为3--&gt;</span><br><span class="line">            &lt;name&gt;dfs.replication&lt;/name&gt; </span><br><span class="line">            &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--定点检查--&gt; </span><br><span class="line">            &lt;name&gt;fs.checkpoint.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;file:///opt/software/hadoop-2.7.7/checkpoint/dfs/cname&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--namenode节点数据（元数据）的存放位置--&gt;</span><br><span class="line">            &lt;name&gt;dfs.name.dir&lt;/name&gt; </span><br><span class="line">            &lt;value&gt;file:///opt/software/hadoop-2.7.7/dfs/namenode_data&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--datanode节点数据（元数据）的存放位置--&gt;</span><br><span class="line">            &lt;name&gt;dfs.data.dir&lt;/name&gt; </span><br><span class="line">            &lt;value&gt;file:///opt/software/hadoop-2.7.7/dfs/datanode_data&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--指定secondarynamenode的web地址--&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; </span><br><span class="line">            &lt;value&gt;hadoop02:50090&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--hdfs文件操作权限,false为不验证--&gt;</span><br><span class="line">            &lt;name&gt;dfs.permissions&lt;/name&gt; </span><br><span class="line">            &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></tbody></table></figure><p>3). mapred-site.xml</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;  </span><br><span class="line">            &lt;!--指定mapreduce运行在yarn上--&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--配置任务历史服务器IPC--&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:10020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--配置任务历史服务器web-UI地址--&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:19888&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></tbody></table></figure><p>4). yarn-site.xml</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--指定yarn的老大resourcemanager的地址--&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:8032&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:8088&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:8030&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:8031&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop01:8033&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--NodeManager获取数据的方式--&gt;</span><br><span class="line">            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--开启日志聚集功能--&gt;</span><br><span class="line">            &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;!--配置日志保留7天--&gt;</span><br><span class="line">            &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></tbody></table></figure><p>5). 创建master文件</p><p>在当前配置文件目录内是不存在master文件的，我们使用<strong>vim</strong>写入内容到master内保存即可</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 hadoop]$ vim master</span><br><span class="line">hadoop01</span><br></pre></td></tr></tbody></table></figure><p>6). 修改slaves文件</p><p>配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 DataNode 服务和 NodeManager 服务都会被启动。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop02</span><br><span class="line">hadoop03</span><br><span class="line">hadoop04</span><br><span class="line">hadoop05</span><br></pre></td></tr></tbody></table></figure><h2 id="七、分发程序"><a href="#七、分发程序" class="headerlink" title="七、分发程序"></a>七、分发程序</h2><p>将 Hadoop 安装包分发到其他4台服务器，分发后建议在这4台服务器上也配置一下 Hadoop 的环境变量。</p><h3 id="7-1-手动分发"><a href="#7-1-手动分发" class="headerlink" title="7.1  手动分发"></a>7.1  手动分发</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将安装包分发到hadoop02</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/hadoop-2.7.7/ anzhaoliang@hadoop02:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将安装包分发到hadoop03</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/hadoop-2.7.7/ anzhaoliang@hadoop03:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将安装包分发到hadoop04</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/hadoop-2.7.7/ anzhaoliang@hadoop04:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将安装包分发到hadoop05</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/hadoop-2.7.7/ anzhaoliang@hadoop05:/opt/software/</span><br></pre></td></tr></tbody></table></figure><h3 id="7-2-shell脚本实现"><a href="#7-2-shell脚本实现" class="headerlink" title="7.2 shell脚本实现"></a>7.2 shell脚本实现</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#回到根目录</span><br><span class="line">[anzhaoliang@hadoop01 ~]$ vim scp-config.sh </span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">description：节点间复制文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">author：anzhaoliang</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">首先判断参数是否存在</span></span><br><span class="line">args=$#</span><br><span class="line">if [ args -eq 0 ];then</span><br><span class="line">    echo "no args"</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=$(basename $p1)</span><br><span class="line">echo faname=$fname</span><br><span class="line"><span class="meta">#</span><span class="bash">获取上级目录到绝对路径</span></span><br><span class="line">pdir=$(cd $(dirname $p1);pwd -P)</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"><span class="meta">#</span><span class="bash">获取当前用户名称</span></span><br><span class="line">user=$(whoami)</span><br><span class="line"><span class="meta">#</span><span class="bash">循环分发</span></span><br><span class="line">for(( host=2;host&lt;4;host++ ));do</span><br><span class="line">    echo "------hadoop0$host------"</span><br><span class="line">    scp -r $pdir/$fname $user@hadoop0$host:$pdir</span><br><span class="line">done</span><br><span class="line">echo "------ALL DONE------"</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ sh scp-config.sh /opt/software/hadoop-2.7.7/</span><br></pre></td></tr></tbody></table></figure><h3 id="7-3-初始化"><a href="#7-3-初始化" class="headerlink" title="7.3 初始化"></a>7.3 初始化</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ hdfs namenode -format</span><br></pre></td></tr></tbody></table></figure><h2 id="八、启动集群"><a href="#八、启动集群" class="headerlink" title="八、启动集群"></a>八、启动集群</h2><p>在 <code>hadoop01</code> 上启动 Hadoop集群。此时 其他四台机器上的相关服务也会被启动：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动dfs服务</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动yarn服务</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动任务历史服务器</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></tbody></table></figure><h2 id="九、查看集群"><a href="#九、查看集群" class="headerlink" title="九、查看集群"></a>九、查看集群</h2><p>在每台服务器上使用 <code>jps</code> 命令查看服务进程，或直接进入 Web-UI 界面进行查看，端口为 <code>50070</code>。可以看到此时有4个可用的 <code>Datanode</code></p><p><img src="/source/images/06.png" alt="image-20210809094113812"></p><p><img src="/source/images/07.png" alt="image-20210809094725673"></p><p>点击 <code>Live Nodes</code> 进入，可以看到每个 <code>DataNode</code> 的详细情况：</p><p><img src="/source/images/08.png" alt="image-20210809094807989"></p><p>接着可以查看 Yarn 的情况，端口号为 <code>8088</code> ：</p><p><img src="/source/images/09.png" alt="image-20210809094505773"></p><h2 id="十、提交任务到集群"><a href="#十、提交任务到集群" class="headerlink" title="十、提交任务到集群"></a>十、提交任务到集群</h2><p>提交作业到集群的方式和单机环境完全一致，这里以提交 Hadoop 内置的计算 Pi 的示例程序为例，在任何一个节点上执行都可以，命令如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#第1个11指的是要运行11次map任务 </span><br><span class="line">#第2个数字指的是每个map任务，要投掷多少次 </span><br><span class="line">[anzhaoliang@hadoop01 ~]$ hadoop jar /opt/software/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 11 24</span><br></pre></td></tr></tbody></table></figure><p>最终计算结果：</p><p><img src="/source/images/10.png" alt="image-20210809095153626"></p><p>Web-UI界面刚才执行的任务状况：</p><p><img src="/source/images/11.png" alt="image-20210809095227885"></p><p>历史记录</p><p><img src="/source/images/12.png" alt="image-20210809095309380"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark集群环境搭建</title>
      <link href="/2021/12/26/spark-ji-qun-fen-bu-shi-an-zhuang/"/>
      <url>/2021/12/26/spark-ji-qun-fen-bu-shi-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark集群环境搭建-Standalone模式"><a href="#Spark集群环境搭建-Standalone模式" class="headerlink" title="Spark集群环境搭建(Standalone模式)"></a>Spark集群环境搭建(Standalone模式)</h1><h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><p>CentOS7、jdk1.8、scala-2.11.12、spark-2.3.3、hadoop-2.7 .7</p><p>hadoop集群</p><h2 id="一、集群规划"><a href="#一、集群规划" class="headerlink" title="一、集群规划"></a>一、集群规划</h2><p><img src="/images/spark/1.jpg"></p><h2 id="二、环境搭建"><a href="#二、环境搭建" class="headerlink" title="二、环境搭建"></a>二、环境搭建</h2><h3 id="2-1上传"><a href="#2-1上传" class="headerlink" title="2.1上传"></a>2.1上传</h3><p>将scala和spark压缩包上传到之前搭建好的hadoop集群容器</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]#docker run -d --name hadoop01 --hostname hadoop01 -P -p 8080:8080 -p 4040:4040 -p 18080:18080  --privileged a4cb95de4776 /usr/sbin/init</span><br><span class="line">[root@master ~]#docker run -d --name hadoop01 --hostname hadoop01 -P -p 50070:50070 -p 8088:8088 -p 19888:19888 -p 8080:8080 -p 4040:4040 -p 18080:18080 -p 9000:9000 --privileged a4cb95de4776 /usr/sbin/init</span><br><span class="line">[root@master ~]#docker run -d --name hadoop01 --hostname hadoop01 -P -p 50070:50070 -p 8088:8088 -p 19888:19888 -p 8080:8080 -p 4040:4040 -p 18080:18080 -p 9000:9000 --privileged a4cb95de4776 /usr/sbin/init</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node2:/home/bigdate# docker cp scala-2.11.12.tgz 3a6e884f5582:/home/anzhaoliang</span><br><span class="line">root@node2:/home/bigdate# docker cp spark-2.3.3-bin-hadoop2.7.tgz 3a6e884f5582:/home/anzhaoliang</span><br></pre></td></tr></tbody></table></figure><h3 id="2-2解压"><a href="#2-2解压" class="headerlink" title="2.2解压"></a>2.2解压</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 解压</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz -C /opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重命名（可选）</span></span><br><span class="line">[anzhaoliang@hadoop01 ~]$ mv /opt/software/spark-2.4.5-bin-hadoop2.7/ /opt/software/spark-2.3.3</span><br></pre></td></tr></tbody></table></figure><h3 id="2-3配置环境变量"><a href="#2-3配置环境变量" class="headerlink" title="2.3配置环境变量"></a>2.3配置环境变量</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ sudo vim /etc/profile#五台主机都配一下</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">configure</span></span><br><span class="line">export JAVA_HOME=/opt/moudle/jdk1.8.0_191</span><br><span class="line">export HADOOP_HOME=/opt/software/hadoop-2.7.7</span><br><span class="line">export SCALA_HOME=/opt/software/scala-2.11.12</span><br><span class="line">export SPARK_HOME=/opt/software/spark-2.3.3-bin-hadoop2.7</span><br><span class="line">export JRE_HOME=${JAVA_HOME}/jre</span><br><span class="line">export CLASSPATH=${JAVA_HOME}/lib:${JRE_HOME}/lib</span><br><span class="line">export PATH=${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SCALA_HOME}/bin:${SPARK_HOME}/bin:$PATH</span><br></pre></td></tr></tbody></table></figure><p>使环境变量生效</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ source /etc/profile.d/env.sh#五台主机都配一下</span><br></pre></td></tr></tbody></table></figure><h3 id="2-4修改配置"><a href="#2-4修改配置" class="headerlink" title="2.4修改配置"></a>2.4修改配置</h3><h4 id="2-4-1修改spark-env-sh"><a href="#2-4-1修改spark-env-sh" class="headerlink" title="2.4.1修改spark-env.sh"></a>2.4.1修改spark-env.sh</h4><p>进入 <code>${SPARK_HOME}/conf</code> 目录下，复制一份<code>spark-env.sh.template</code>文件进行更改</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 conf]$ cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/moudle/jdk1.8.0_301</span><br><span class="line">export SCALA_HOME=/opt/moudle/scala-2.11.12</span><br><span class="line">SPARK_MASTER_HOST=hadoop01</span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认端口就是7077, 可以省略不配</span></span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></tbody></table></figure><h4 id="2-4-2修改-slaves-文件"><a href="#2-4-2修改-slaves-文件" class="headerlink" title="2.4.2修改 slaves 文件"></a>2.4.2修改 slaves 文件</h4><p>添加 worker 节点,这里配置了四个工作节点</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 conf]$ cp slaves.template slaves</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop02</span><br><span class="line">hadoop03</span><br><span class="line">hadoop04</span><br><span class="line">hadoop05</span><br></pre></td></tr></tbody></table></figure><h3 id="2-5分发"><a href="#2-5分发" class="headerlink" title="2.5分发"></a>2.5分发</h3><p>如果之前配置过scala环境直接执行分发spark代码块</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将scala-2.11.12文件夹分发到hadoop02</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop02:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将scala-2.11.12文件夹分发到hadoop03</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop03:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将scala-2.11.12文件夹分发到hadoop04</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop04:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将scala-2.11.12文件夹分发到hadoop05</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop05:/opt/software/</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将spark-2.3.3文件夹分发到hadoop02</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop02:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-2.3.3文件夹分发到hadoop03</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop03:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-2.3.3文件夹分发到hadoop04</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop04:/opt/software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-2.3.3文件夹分发到hadoop05</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r /opt/software/spark-2.3.3/ anzhaoliang@hadoop05:/opt/software/</span><br></pre></td></tr></tbody></table></figure><h2 id="三、启动集群"><a href="#三、启动集群" class="headerlink" title="三、启动集群"></a>三、启动集群</h2><p><strong>在hadoop01上启动Spark集群</strong></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入/opt/software/spark-2.3.3/sbin目录并启动集群</span></span><br><span class="line">[xiaokang@hadoop01 sbin]$ ./start-all.sh</span><br></pre></td></tr></tbody></table></figure><p>jps进程查看</p><p><img src="/images/spark/2.png" alt="image-20210810095303528"></p><p>webui查看</p><p><img src="/images/spark/3.png" alt="image-20210810095443250"></p><h2 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h2><h3 id="4-1计算PI"><a href="#4-1计算PI" class="headerlink" title="4.1计算PI"></a>4.1计算PI</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 sbin]$ spark-submit --master spark://hadoop01:7077 --executor-memory 1G --total-executor-cores 8 --executor-cores 2 --class org.apache.spark.examples.SparkPi /opt/software/spark-2.3.3/examples/jars/spark-examples_2.11-2.3.3.jar 1000</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/spark/4.png" alt="image-20210810111005884"></p><p>Webui查看资源分配情况</p><p><img src="/images/spark/5.png" alt="image-20210810111148687"></p><h2 id="五、配置任务历史服务器"><a href="#五、配置任务历史服务器" class="headerlink" title="五、配置任务历史服务器"></a>五、配置任务历史服务器</h2><p>在 Spark-shell 没有退出之前， 我们是可以看到正在执行的任务的日志情况：<code>http://ip:4040</code>，但是退出 Spark-shell 之后， 执行的所有任务记录全部丢失。所以需要配置任务的历史服务器, 方便在任何需要的时候去查看日志。</p><h3 id="5-1-修改spark-defaults-conf文件"><a href="#5-1-修改spark-defaults-conf文件" class="headerlink" title="5.1 修改spark-defaults.conf文件"></a>5.1 修改<code>spark-defaults.conf</code>文件</h3><p>进入 <code>${SPARK_HOME}/conf</code> 目录下，复制一份<code>spark-defaults.conf.template</code>文件进行更改</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 conf]$ cp spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">[anzhaoliang@hadoop01 conf]$ vim spark-defaults.conf</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     spark://hadoop01:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://hadoop01:9000/spark-jobhistory#此处spark-jobhistory目录必须已经存在</span><br></pre></td></tr></tbody></table></figure><p><strong>注意：</strong> <code>hdfs://hadoop01:9000/spark-jobhistory</code> 目录必须提前存在, 名称可自定义。,然后再修改spark-defaults.conf</p><p>在HDFS上创建<code>spark-jobhistory</code>目录</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ hdfs dfs -mkdir /spark-jobhistory</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/spark/6.png" alt="image-20210810111648537"></p><h3 id="5-2-修改spark-env-sh文件，添加如下配置"><a href="#5-2-修改spark-env-sh文件，添加如下配置" class="headerlink" title="5.2 修改spark-env.sh文件，添加如下配置"></a>5.2 修改<code>spark-env.sh</code>文件，添加如下配置</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 conf]$ vim spark-env.sh</span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop01:9000/spark-jobhistory"</span><br></pre></td></tr></tbody></table></figure><h3 id="5-3-分发配置文件"><a href="#5-3-分发配置文件" class="headerlink" title="5.3 分发配置文件"></a>5.3 分发配置文件</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将spark-defaults.conf分发到hadoop02</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-defaults.conf anzhaoliang@hadoop02:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-defaults.conf分发到hadoop03</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-defaults.conf anzhaoliang@hadoop03:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-defaults.conf分发到hadoop04</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-defaults.conf anzhaoliang@hadoop04:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-defaults.conf分发到hadoop05</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-defaults.conf anzhaoliang@hadoop05:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-env.sh分发到hadoop02</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-env.sh anzhaoliang@hadoop02:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-env.sh分发到hadoop03</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-env.sh anzhaoliang@hadoop03:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-env.sh分发到hadoop04</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-env.sh anzhaoliang@hadoop04:/opt/software/spark-2.3.3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将spark-env.sh分发到hadoop05</span></span><br><span class="line">[anzhaoliang@hadoop01 hadoop]$ sudo scp -r spark-env.sh anzhaoliang@hadoop05:/opt/software/spark-2.3.3/conf</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-启动历史服务器"><a href="#5-4-启动历史服务器" class="headerlink" title="5.4 启动历史服务器"></a>5.4 启动历史服务器</h3><p>先停止hadoop和spark集群</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 sbin]$ stop-all.sh</span><br><span class="line">[anzhaoliang@hadoop01 sbin]$ mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line">[anzhaoliang@hadoop01 sbin]$ ./stop-all.sh</span><br><span class="line">    [anzhaoliang@hadoop01 sbin]$ ./stop-history-server.sh</span><br></pre></td></tr></tbody></table></figure><p>首先启动HDFS，然后启动Spark集群，最后启动任务历史服务器</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 sbin]$ start-all.sh</span><br><span class="line">[anzhaoliang@hadoop01 sbin]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">[anzhaoliang@hadoop01 sbin]$ ./start-all.sh</span><br><span class="line">[anzhaoliang@hadoop01 sbin]$ ./start-history-server.sh</span><br></pre></td></tr></tbody></table></figure><p><strong>jps进程查看</strong></p><p><img src="/images/spark/7.png" alt="image-20210810103148022"></p><p><strong>WebUI查看</strong></p><p><img src="/images/spark/8.png" alt="image-20210810113037344"></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[anzhaoliang@hadoop01 ~]$ spark-shell --master spark://hadoop01:7077</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
